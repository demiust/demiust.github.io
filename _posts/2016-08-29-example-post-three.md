---
title: Density-based novelty detection
categories: 
- General
excerpt: |
  Density-based novelty detection (Gaussian Density Estimation, Mixture of Gaussian, Kernel Density Estimation, LOF)
feature_text: |
  ## Density-based novelty detection
  이 글은 고려대학교 강필성 교수님의 Business Analytics 강의를 참조하였습니다.
feature_image: "https://picsum.photos/2560/600?image=733"
image: "https://picsum.photos/2560/600?image=733"
use_math: true
---

<h2> Novelty Detection이란? </h2> 

Novelty detection은 일반적인 데이터의 경향과는 다른 특이한 혹은 이상한 데이터(novel data, outliers)를 찾아내는 기술입니다. 예를 들어 갑자기 어떤 신용카드가 여태까지 쓰이지 않던 장소에서 쓰이지 않던 방식으로 쓰였다면 이것은 '이상한 데이터'라고 할 수 있습니다. Novelty detection은 이런 식의 이상성을 발견하고자 하는 알고리즘입니다.

<em> “Observations that deviate so much from other observations as to arouse suspicions that they were __generated by a different mechanism__ (Hawkins, 1980)”
“Instances that their __true probability density is very low__ (Harmelinget al., 2006)” </em>

위에서 정의된 것처럼 novel data(outliers)는 일반적인 데이터와 차이가 많이 나는 관측 데이터라고 볼 수 있습니다. 여기서 재밌는 점은 novel data는 __noise data와 다르다__ 는 점입니다. Noise는 데이터 전처리 과정에서 제거해줘야 할 데이터이기 때문에 novelty detection 이전에 제거하게 됩니다. 하지만 novel data는 일반적인 데이터를 만들어낸 메커니즘을 위반하는 특이한 케이스의 관측치들이기 때문에 novelty detection에서 찾아내고자 하는 관측치입니다.

<h3> Novel data의 종류 </h3>
Novel data에는 크게 세 종류가 있습니다. 
<br>
* Global Outlier
{% include figure.html image="/images/image1.png"%}

우리가 보는 일반적인 “outlier”라고 할 수 있습니다. 단순히 일반적인 관측치와 동떨어진 관측치들입니다.

* Contextual outlier(local outlier)
{% include figure.html image="/images/image2.png"%}

특정 상황 혹은 맥락 정보를 고려해 “outlier”가 되는 경우입니다. 예를 들면, 알래스카의 영상 30도는 outlier라고 볼 수 있지만, 사하라 사막의 영상 30도는 outlier가 아닐 수 있습니다.

* Collective outlier

개별적인 관측치들이 outlier가 아니더라도, 데이터의 부분집합이 집단적으로 전체 데이터로부터 동떨어진 움직임이 있다면 이것을 collective outlier라고 합니다. 대표적으로 Dos attack이 있습니다.

<h3> Classification과의 차이 </h3>
Novelty Detection은 Classification과 차이가 있습니다. 

{% include figure.html image="/images/image3.png"%}
왼쪽의 그림에서처럼 Classification은 X와 O를 각각 X의 경우에는 “빨간색”의 범주로, O의 경우에는 “파란색”의 범주로 학습시킵니다. 즉, 각각의 범주를 학습시킵니다.
반면 오른쪽의 그림에서처럼 Novelty detection은 파란색만 데이터로 활용해 훈련해서, 결과적으로 파란색(정상) 범주에 포함되는지 포함되지 않는지만을 알아내는 것이 목적입니다.

Class Imbalance가 크고, minority class example의 개수가 충분하지 않을 때 우리는 novelty detection 알고리즘을 사용하고, 만약 class간의 imbalance가 크더라도 minority class example 데이터의 절대량이 크다면 비율을 맞춰서 classification 방식으로 훈련하는 것이 적합합니다. 즉, novelty detection은 classification 방식이 불가능할 때 대안 방법으로 사용할 수 있습니다.

<h3> Performance measure </h3>
Novelty detection 알고리즘의 평가는 다음과 같이 합니다.

{% include figure.html image="/images/image4.png"%}
우선 normal data를 novel data로 분류한 경우와, novel data를 normal data로 분류한 경우가 적을 수록 이상한 데이터를 잘 가려내었다고 판단할 수 있습니다.

{% include figure.html image="/images/image5.png"%}
normal data를 novel data로 분류한 경우의 에러는 False Rejection Rate(FRR)로 표현할 수 있고, novel data를 normal data로 분류한 경우의 에러는 False Acceptance Rate (FAR)로 표현할 수 있습니다.

{% include figure.html image="/images/image6.png"%}
FRR과 FAR을 두 축으로 해서 FRR이 FAR과 같아질 때를 Equal error rate(EER)이라고 하고, FRR-FAR curve의 밑부분의 면적을 Integrated Error(IE)라고 합니다. Novelty detection 알고리즘은 EER과 IE가 낮을수록 좋다고 평가할 수 있습니다.

<h2> Density-based Novelty Detection </h2> 

Density-based 방식은 기존에 가지고 있는 정상 데이터의 분포를 사용하여 outlier를 찾아내는 방식입니다. 기존에 있는 데이터를 사용하여 알고리즘을 학습하기 때문에 supervised learning이라고 할 수 있습니다.

(그림7)
위 그림에서 보시다시피, 우리가 갖고 있는 정상 데이터 분포가 있을 경우에 빨간색 데이터가 들어온다면 이는 outlier라고 할 수 있습니다. 이렇듯 정상 데이터만을 사용하여 추정한 분포를 가지고 이상치를 탐지할 수 있습니다.

앞으로 설명할 밀도 기반 이상치 탐지 기법은 총 4개입니다.<br>
* Gaussian Density Estiation
* Mixture of Gaussian Density Estimation
* Kernel-density Estimation
* Local Outlier Factors (LOF)

<h2> Gaussian Density Estimation </h2>
가우시안 밀도 추정 방법은 우리가 가진 정상 데이터의 분포가 기본적으로 가우시안 분포(정규 분포)를 따른다고 가정하는 방법입니다. 

image8
$$p(x)\quad =\quad \frac { 1 }{ { 2\pi  }^{ { d }/{ 2 } }{ \Sigma  }^{ { 1 }/{ 2 } } } exp\left[ \frac { 1 }{ 2 } { (x-\mu ) }^{ T }{ \Sigma  }^{ -1 }(x-\mu ) \right]$$

이 식은 우리가 익히 보아온 가우시안 분포를 표현한 식입니다. 여기서 각각 mu와 sigma는 아래의 식과 같습니다.

$$\mu \quad =\quad \frac { 1 }{ n } \sum _{ { x }_{ i }\in { X }^{ + } }^{  }{ { x }_{ i } } \quad (mean\quad vector)$$
$$\Sigma \quad =\quad \frac { 1 }{ n } \sum _{ { x }_{ i }\in { X }^{ + } }^{  }{ ({ x }_{ i }-\mu ){ ({ x }_{ i }-\mu ) }^{ T } } \quad (covariance\quad matrix)$$

여기서 X+는 정상 데이터를 의미합니다.

<h2>Maximum likelihood estimation</h2>
데이터의 각 값들이 나올 확률을 가장 잘 설명하는 분포가 그 데이터의 가우시안 분포라고 할 수 있습니다. 왜냐하면 실제 관측값이 측정될 각각의 확률들을 곱했을 때 최대가 되게끔 하는 분포가 그 데이터의 가우시안 분포이기 때문입니다. 이를 밑의 식으로 표현할 수 있습니다.

$$ L=\prod _{ i=1 }^{ N }{ P({ x }_{ i }|\mu ,{ \sigma  }^{ 2 }) } =\prod _{ i=1 }^{ N }{ \frac { 1 }{ \sqrt { 2\pi  } \sigma  } exp(-\frac { ({ x }_{ i }-\mu )^{ 2 } }{ 2{ \sigma  }^{ 2 } } ) }$$

여기에 로그를 씌우면 밑의 식과 같이 됩니다.
$$\log { L } =-\frac { 1 }{ 2 } \sum _{ i=1 }^{ N }{ \frac { ({ x }_{ i }-\mu )^{ 2 } }{ { \sigma  }^{ 2 } }  } -\frac { N }{ 2 } \log { (2\pi { \sigma  }^{ 2 }) }$$

식을 더 쉽게 표현하기 위해 $\gamma =\frac { 1 }{ { \sigma  }^{ 2 } }$로 바꿔서 표현하면 다음과 같습니다.

$$ \log { L } =-\frac { 1 }{ 2 } \sum _{ i=1 }^{ N }{ \gamma ({ x }_{ i }-\mu )^{ 2 } } -\frac { N }{ 2 } \log { (2\pi ) } +\frac { N }{ 2 } log(\gamma )$$

Log-likelyhood는 아시다시피 위로 볼록한 함수입니다. 이러한 이유 때문에 미지수인 $\mu $와 $\gamma $에 대해 일차 미분을 해서 최적값을 구할 수 있습니다. 

$$\frac { \partial log{ L } }{ \partial \mu  } =\gamma \sum _{ i=1 }^{ N }{ ({ x }_{ i }-\mu ) } =0 \rightarrow \quad \mu =\frac { 1 }{ N } \sum _{ i=1 }^{ N }{ { x }_{ i } }$$
$$\frac { \partial log{ L } }{ \partial \gamma  } =-\frac { 1 }{ 2 } \sum _{ i=1 }^{ N }{ ({ x }_{ i }-\mu )^{ 2 } } +\frac { N }{ 2\gamma  } =0 \rightarrow \quad { \sigma  }^{ 2 }=\frac { 1 }{ N } \sum _{ i=1 }^{ N }{ { (x }_{ i }-\mu )^{ 2 } }
$$

위의 식을 통해서 우리가 가진 데이터의 평균과 분산이 정규분포의 평균과 분산과 같아지는 것을 확인할 수 있었습니다.

class Gaussian:
    def __init__(self, mu, sigma):
        self.mu = mu
        self.sigma = sigma

    def pdf(self, data): # 가우시안 분포 pdf 값 return
        u = (data - self.mu) / abs(self.sigma)
        y = (1 / (sqrt(2 * pi) * abs(self.sigma))) * exp(-u * u / 2)
        return y

# 가우시안 분포 그리기
x = np.linspace(3,9,200)
g_single = stats.norm(best.mu, best.sigma).pdf(x)
sns.distplot(y, bins=20, kde = False, norm_hist= True)
plt.plot(x,g_single, label = 'Single Gaussian')
plt.legend()

print(y[0:5])

#정상 boundary 외 outlier filtering
n = 0
b=0
for i in range(0,y.shape[0]):
    if (stats.norm(best.mu, best.sigma).pdf(y[i])) >0.05 and (stats.norm(best.mu, best.sigma).pdf(y[i])) < 0.995:
        print(y[i],"= normal")
        n=n+1
    else:
        print(y[i],"=abnormal")
        b=b+1

print("normal=",n)
print(“abnormal=",b)

코드 설명

—
Spherical type
그림

Diagonal type
그림

Full type
그림

2. - Mixture of Gaussian
그림

위에서는 하나의 가우시안 분포로 전체 데이터를 설명하고자 했다면, 
혼합 가우시안 분포는 여러개의 가우시안 분포의 조합으로 데이터를 추정하고자 하는 것입니다.

data가 normal data일 확률은 이렇습니다.

(식)

(파라미터 설명)


EM algorithm

혼합 가우시안 모델은 Expectation, Maximization 과정을 통해 최적값을 찾아나가야 합니다. 

E step은 (식)

M-step은 (식)

-코드 (이것은 scikil-learn 등의 하이레벨 써도 될듯) _ https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html 참고

3. - Kernel Density 

커널 밀도 추정은 데이터가 가우시안 분포와 같은 특정 분포를 따르지 않는다고 가정하는 방법입니다. 

Parzen
설명만 하고
코드는 간단하게만 제시하기 (high-level)

5. - LOF

설명


코드




$ \frac { 3 }{ 4 } $

$$
K(a,b) = \int \mathcal{D}x(t) \exp(2\pi i S[x]/\hbar)
$$
